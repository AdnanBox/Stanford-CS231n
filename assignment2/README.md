In this assignment you will practice writing backpropagation code, and training Neural Networks and Convolutional Neural Networks. The goals of this assignment are as follows:
- understand **Neural Networks** and how they are arranged in layered architectures
- understand and be able to implement (vectorized) **backpropagation**
- implement various **update rules** used to optimize Neural Networks
- implement **Batch Normalization** and **Layer Normalization** for training deep networks
- implement **Dropout** to regularize networks
- understand the architecture of **Convolutional Neural Networks** and get practice with training these models on data
- gain experience with a major deep learning framework, such as **TensorFlow** or **PyTorch**.
<br />
<br />  

## Q1. Fully-Connected Neural Network
The IPython notebook **FullyConnectedNets.ipynb** will introduce you to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models you will implement several popular update rules.

## Q2. Batch Normalization
In the IPython notebook **BatchNormalization.ipynb** you will implement batch normalization, and use it to train deep fully-connected networks.

## Q3. Dropout
The IPython notebook **Dropout.ipynb** will help you implement Dropout and explore its effects on model generalization.

## Q4. Convolutional Networks
In the IPython Notebook **ConvolutionalNetworks.ipynb** you will implement several new layers that are commonly used in convolutional networks.

## Q5. TensorFlow on CIFAR-10
Open up **TensorFlow.ipynb**. There, you will learn how the framework works, culminating in training a convolutional network of your own design on CIFAR-10 to get the best performance you can.
<br /> 
<br /> 
<br /> 
<br /> 
**NOTE:** Details about this assignment can be found [on the course webpage](https://cs231n.github.io/assignments2019/assignment1/).
